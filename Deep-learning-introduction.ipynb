{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLHTNfSclli"
   },
   "source": [
    "**Секція 1. Логістична регресія з нуля.**\n",
    "\n",
    "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
    "\n",
    "Давайте нагадаємо основні формули для логістичної регресії.\n",
    "\n",
    "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
    "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
    "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
    "- $ b $ — це зміщення (bias).\n",
    "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
    "\n",
    "### Як обчислюється сигмоїдна функція:\n",
    "\n",
    "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
    "\n",
    "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
    "\n",
    "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
    "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtOYB-RHfc_r"
   },
   "source": [
    "1.\n",
    "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "3BNXSR-VdYKQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QLKZ77x4v_-v"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KjoeaDrk6fO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKzbJKfOgGV8"
   },
   "source": [
    "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aXhKw6Tdj1-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25853efded0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eApcB7eb6h9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6614, 0.2669, 0.0617], requires_grad=True)\n",
      "tensor([0.6213], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(3, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYGxNGTaf5s6"
   },
   "source": [
    "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
    "\n",
    "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
    "  - обчсилення $e^x$: `torch.exp(x)`\n",
    "  - обчсилення $log(x)$: `torch.log(x)`\n",
    "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
    "\n",
    "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
    "\n",
    "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pSz2j4Fh6jBv"
   },
   "outputs": [],
   "source": [
    "# Визначаємо модель\n",
    "def model(x, w, b, threshold=0.5):\n",
    "    z = torch.matmul(x, w) + b\n",
    "    probs = 1 / (1 + torch.exp(-z))\n",
    "    preds = (probs > threshold).float() \n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Генеруємо передбачення\n",
    "preds, probs = model(inputs, w, b)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделька явно предиктись все як один класс і при чому ймоврініть чомусь завжди рівна 100%. Можливо підхід з рандомними вагами і зсувом треба переглянути."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2AGM0Mb2yHa"
   },
   "source": [
    "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
    "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1bWlovvx6kZS"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(predicted_probs, true_labels):\n",
    "    # Уникаємо обчислень log(0) або log(1) шляхом використання невеликого значення eps\n",
    "    eps = 1e-7\n",
    "    predicted_probs = torch.clamp(predicted_probs, eps, 1 - eps)\n",
    "    \n",
    "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376953601837158\n"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(probs, targets)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKpQxdHi1__"
   },
   "source": [
    "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YAbXUNSJ6mCl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6614, 0.2669, 0.0617], requires_grad=True)\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Обчислимо gradients\n",
    "loss.backward()\n",
    "\n",
    "# Градієнти вагів\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6213], requires_grad=True)\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDN1t1RujQsK"
   },
   "source": [
    "**Що сталось?**\n",
    "\n",
    "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
    "\n",
    "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "\n",
    "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
    "\n",
    "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
    "\n",
    "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
    "\n",
    "**Що ж робити?**\n",
    "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
    "\n",
    "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
    "\n",
    "А я пишу пояснення, чому просто не зробити\n",
    "\n",
    "```\n",
    "w = torch.randn(1, 3, requires_grad=True)/1000\n",
    "b = torch.randn(1, requires_grad=True)/1000\n",
    "```\n",
    "\n",
    "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
    "\n",
    "1. **Що таке листовий тензор**\n",
    "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
    "\n",
    "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
    "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
    "\n",
    "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
    "\n",
    "3. **Чому важливо залишити тензор листовим**\n",
    "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
    "\n",
    "**Висновок:**\n",
    "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOPSQyttpVjO"
   },
   "source": [
    "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-EBOJ3tsnRaD"
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "w = torch.randn(3, requires_grad=True)  # Листовий тензор\n",
    "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
    "\n",
    "# in-place операції\n",
    "w.data = w.data / 1000\n",
    "b.data = b.data / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.6135e-04, 2.6692e-04, 6.1677e-05])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-JwXiSpX6orh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Генеруємо передбачення\n",
    "preds, probs = model(inputs, w, b)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6857479810714722\n"
     ]
    }
   ],
   "source": [
    "# Рахуємо функцію втрат\n",
    "loss = binary_cross_entropy(probs, targets)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.6135e-04, 2.6692e-04, 6.1677e-05], requires_grad=True)\n",
      "tensor([-6.6817, -6.7453, -4.3082])\n"
     ]
    }
   ],
   "source": [
    "# Обчислимо gradients\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# Градієнти вагів\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0006], requires_grad=True)\n",
      "tensor([-0.0794])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCdi44IT334o"
   },
   "source": [
    "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
    "\n",
    "  1. Генерація прогнозів\n",
    "  2. Обчислення втрат\n",
    "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
    "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
    "  5. Скидання градієнтів на нуль\n",
    "\n",
    "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mObHPyE06qsO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6857479810714722\n",
      "Epoch 10, Loss: 0.6780029535293579\n",
      "Epoch 20, Loss: 0.6753175854682922\n",
      "Epoch 30, Loss: 0.6742188334465027\n",
      "Epoch 40, Loss: 0.6737637519836426\n",
      "Epoch 50, Loss: 0.6735718250274658\n",
      "Epoch 60, Loss: 0.673488438129425\n",
      "Epoch 70, Loss: 0.6734501123428345\n",
      "Epoch 80, Loss: 0.6734302639961243\n",
      "Epoch 90, Loss: 0.673418402671814\n",
      "Epoch 100, Loss: 0.6734099388122559\n",
      "Epoch 110, Loss: 0.6734029650688171\n",
      "Epoch 120, Loss: 0.6733965873718262\n",
      "Epoch 130, Loss: 0.6733908653259277\n",
      "Epoch 140, Loss: 0.6733851432800293\n",
      "Epoch 150, Loss: 0.6733798384666443\n",
      "Epoch 160, Loss: 0.6733747124671936\n",
      "Epoch 170, Loss: 0.6733695864677429\n",
      "Epoch 180, Loss: 0.6733646988868713\n",
      "Epoch 190, Loss: 0.6733599305152893\n",
      "Epoch 200, Loss: 0.6733553409576416\n",
      "Epoch 210, Loss: 0.6733508110046387\n",
      "Epoch 220, Loss: 0.6733464598655701\n",
      "Epoch 230, Loss: 0.6733422875404358\n",
      "Epoch 240, Loss: 0.6733381152153015\n",
      "Epoch 250, Loss: 0.6733341217041016\n",
      "Epoch 260, Loss: 0.6733302474021912\n",
      "Epoch 270, Loss: 0.6733264923095703\n",
      "Epoch 280, Loss: 0.673322856426239\n",
      "Epoch 290, Loss: 0.6733193397521973\n",
      "Epoch 300, Loss: 0.6733158826828003\n",
      "Epoch 310, Loss: 0.6733124256134033\n",
      "Epoch 320, Loss: 0.6733092665672302\n",
      "Epoch 330, Loss: 0.6733061075210571\n",
      "Epoch 340, Loss: 0.6733030676841736\n",
      "Epoch 350, Loss: 0.67330002784729\n",
      "Epoch 360, Loss: 0.6732971668243408\n",
      "Epoch 370, Loss: 0.6732943654060364\n",
      "Epoch 380, Loss: 0.6732916235923767\n",
      "Epoch 390, Loss: 0.6732889413833618\n",
      "Epoch 400, Loss: 0.6732864379882812\n",
      "Epoch 410, Loss: 0.6732839345932007\n",
      "Epoch 420, Loss: 0.6732814908027649\n",
      "Epoch 430, Loss: 0.6732791066169739\n",
      "Epoch 440, Loss: 0.6732768416404724\n",
      "Epoch 450, Loss: 0.6732745170593262\n",
      "Epoch 460, Loss: 0.6732723712921143\n",
      "Epoch 470, Loss: 0.6732703447341919\n",
      "Epoch 480, Loss: 0.6732682585716248\n",
      "Epoch 490, Loss: 0.6732662916183472\n",
      "Epoch 500, Loss: 0.6732643246650696\n",
      "Epoch 510, Loss: 0.6732624173164368\n",
      "Epoch 520, Loss: 0.6732606291770935\n",
      "Epoch 530, Loss: 0.673258900642395\n",
      "Epoch 540, Loss: 0.6732571125030518\n",
      "Epoch 550, Loss: 0.673255443572998\n",
      "Epoch 560, Loss: 0.6732537746429443\n",
      "Epoch 570, Loss: 0.6732521653175354\n",
      "Epoch 580, Loss: 0.6732507348060608\n",
      "Epoch 590, Loss: 0.6732491254806519\n",
      "Epoch 600, Loss: 0.673247754573822\n",
      "Epoch 610, Loss: 0.6732463240623474\n",
      "Epoch 620, Loss: 0.6732449531555176\n",
      "Epoch 630, Loss: 0.6732435822486877\n",
      "Epoch 640, Loss: 0.6732422709465027\n",
      "Epoch 650, Loss: 0.6732409596443176\n",
      "Epoch 660, Loss: 0.6732397675514221\n",
      "Epoch 670, Loss: 0.6732385754585266\n",
      "Epoch 680, Loss: 0.6732373833656311\n",
      "Epoch 690, Loss: 0.6732362508773804\n",
      "Epoch 700, Loss: 0.6732351779937744\n",
      "Epoch 710, Loss: 0.6732341051101685\n",
      "Epoch 720, Loss: 0.6732330322265625\n",
      "Epoch 730, Loss: 0.6732320189476013\n",
      "Epoch 740, Loss: 0.6732309460639954\n",
      "Epoch 750, Loss: 0.6732300519943237\n",
      "Epoch 760, Loss: 0.6732290387153625\n",
      "Epoch 770, Loss: 0.6732282042503357\n",
      "Epoch 780, Loss: 0.6732273101806641\n",
      "Epoch 790, Loss: 0.6732264161109924\n",
      "Epoch 800, Loss: 0.6732256412506104\n",
      "Epoch 810, Loss: 0.6732248067855835\n",
      "Epoch 820, Loss: 0.6732239723205566\n",
      "Epoch 830, Loss: 0.6732231974601746\n",
      "Epoch 840, Loss: 0.6732224225997925\n",
      "Epoch 850, Loss: 0.6732215881347656\n",
      "Epoch 860, Loss: 0.6732209920883179\n",
      "Epoch 870, Loss: 0.6732202172279358\n",
      "Epoch 880, Loss: 0.6732195019721985\n",
      "Epoch 890, Loss: 0.673218846321106\n",
      "Epoch 900, Loss: 0.6732181310653687\n",
      "Epoch 910, Loss: 0.6732175350189209\n",
      "Epoch 920, Loss: 0.6732169389724731\n",
      "Epoch 930, Loss: 0.6732164025306702\n",
      "Epoch 940, Loss: 0.6732156872749329\n",
      "Epoch 950, Loss: 0.6732151508331299\n",
      "Epoch 960, Loss: 0.6732145547866821\n",
      "Epoch 970, Loss: 0.6732141375541687\n",
      "Epoch 980, Loss: 0.6732134819030762\n",
      "Epoch 990, Loss: 0.6732130646705627\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Генеруємо передбачення\n",
    "    preds, probs = model(inputs, w, b)\n",
    "    \n",
    "    # Рахуємо функцію втрат\n",
    "    loss = binary_cross_entropy(probs, targets)\n",
    "    \n",
    "    # Обчислюємо градієнти\n",
    "    loss.backward()\n",
    "    \n",
    "    # Оновлюємо ваги та зміщення\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        \n",
    "        # Скидаємо градієнти\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    \n",
    "    # Виводимо втрати для кожної 10 епохи\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights: tensor([0.0032, 0.0008, 0.0013], requires_grad=True)\n",
      "Final bias: tensor([0.0007], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Виведемо фінальні значення вагів та зміщення\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Final bias:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions: tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "final_preds, final_probs = model(inputs, w, b)\n",
    "print(\"Final predictions:\", final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, що 1000 епох для навчання забагато, так як після 100 епох, функція втрат майже не міняється.  А такоє ми все ще погано предіктимо, тому всі дані асайнимо до 1 класу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuRhlyF9qAia"
   },
   "source": [
    "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
    "\n",
    "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
    "\n",
    "Даних у нас буде побільше - тож, визначаємо нові масиви."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "IX8Bhm74rV4M"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X2dV30KtAPu"
   },
   "source": [
    "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "chrvMfBs6vjo"
   },
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Визначаємо dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMFaa8suOd3"
   },
   "source": [
    "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "ZCsRo5Mx6wEI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[102.,  43.,  37.],\n",
       "         [ 87., 134.,  58.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 73.,  67.,  43.],\n",
       "         [ 73.,  67.,  43.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.]])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Визначаємо data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymcQOo_hum6I"
   },
   "source": [
    "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
    "\n",
    "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
    "\n",
    "  Створіть екземпляр класу `LogReg` в змінній `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "EyAwhTBW6xxz"
   },
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid() # Activation function\n",
    "\n",
    "    # Perform the computation\n",
    "    def forward(self, x):\n",
    "        z = self.linear(x)\n",
    "        probs = self.sigmoid(z)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogReg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RflV7xeVyoJy"
   },
   "source": [
    "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "3QCATPU_6yfa"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.865747451782227\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3396e-07],\n",
       "        [1.6896e-08],\n",
       "        [4.0041e-08],\n",
       "        [7.8229e-09],\n",
       "        [5.5783e-07],\n",
       "        [8.3396e-07],\n",
       "        [1.6896e-08],\n",
       "        [4.0041e-08],\n",
       "        [7.8229e-09],\n",
       "        [5.5783e-07],\n",
       "        [8.3396e-07],\n",
       "        [1.6896e-08],\n",
       "        [4.0041e-08],\n",
       "        [7.8229e-09],\n",
       "        [5.5783e-07]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виглядає так, як наче ми усі предікти робимо як клас 0. А отде моделька навчилась погано."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-WrYnKzMzq"
   },
   "source": [
    "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "cEHQH9qE626k"
   },
   "outputs": [],
   "source": [
    "# Модифікована функцію fit для відстеження втрат\n",
    "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ініціалізуємо акумулятор для втрат\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            # Генеруємо передбачення\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Обчислюємо втрати\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Очищення градієнтів\n",
    "            opt.zero_grad()\n",
    "            loss.backward()  # Обчислення градієнтів\n",
    "            opt.step()  # Оновлення ваг\n",
    "\n",
    "            # Накопичуємо втрати\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Обчислюємо середні втрати для епохи\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # Виводимо підсумок епохи\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 9.7613\n",
      "Epoch [20/1000], Loss: 6.1834\n",
      "Epoch [30/1000], Loss: 2.7785\n",
      "Epoch [40/1000], Loss: 0.7222\n",
      "Epoch [50/1000], Loss: 0.4441\n",
      "Epoch [60/1000], Loss: 0.3591\n",
      "Epoch [70/1000], Loss: 0.2637\n",
      "Epoch [80/1000], Loss: 0.1229\n",
      "Epoch [90/1000], Loss: 0.3501\n",
      "Epoch [100/1000], Loss: 0.3670\n",
      "Epoch [110/1000], Loss: 0.1062\n",
      "Epoch [120/1000], Loss: 0.1469\n",
      "Epoch [130/1000], Loss: 0.2364\n",
      "Epoch [140/1000], Loss: 0.3252\n",
      "Epoch [150/1000], Loss: 0.2954\n",
      "Epoch [160/1000], Loss: 0.2315\n",
      "Epoch [170/1000], Loss: 0.2357\n",
      "Epoch [180/1000], Loss: 0.2913\n",
      "Epoch [190/1000], Loss: 0.2352\n",
      "Epoch [200/1000], Loss: 0.3707\n",
      "Epoch [210/1000], Loss: 0.2307\n",
      "Epoch [220/1000], Loss: 0.2387\n",
      "Epoch [230/1000], Loss: 0.1530\n",
      "Epoch [240/1000], Loss: 0.2302\n",
      "Epoch [250/1000], Loss: 0.0971\n",
      "Epoch [260/1000], Loss: 0.0971\n",
      "Epoch [270/1000], Loss: 0.1486\n",
      "Epoch [280/1000], Loss: 0.2296\n",
      "Epoch [290/1000], Loss: 0.3243\n",
      "Epoch [300/1000], Loss: 0.1519\n",
      "Epoch [310/1000], Loss: 0.1821\n",
      "Epoch [320/1000], Loss: 0.2326\n",
      "Epoch [330/1000], Loss: 0.0929\n",
      "Epoch [340/1000], Loss: 0.2321\n",
      "Epoch [350/1000], Loss: 0.2358\n",
      "Epoch [360/1000], Loss: 0.2356\n",
      "Epoch [370/1000], Loss: 0.2317\n",
      "Epoch [380/1000], Loss: 0.1006\n",
      "Epoch [390/1000], Loss: 0.1848\n",
      "Epoch [400/1000], Loss: 0.1426\n",
      "Epoch [410/1000], Loss: 0.4488\n",
      "Epoch [420/1000], Loss: 0.4485\n",
      "Epoch [430/1000], Loss: 0.3675\n",
      "Epoch [440/1000], Loss: 0.0991\n",
      "Epoch [450/1000], Loss: 0.3671\n",
      "Epoch [460/1000], Loss: 0.2845\n",
      "Epoch [470/1000], Loss: 0.1458\n",
      "Epoch [480/1000], Loss: 0.3123\n",
      "Epoch [490/1000], Loss: 0.4005\n",
      "Epoch [500/1000], Loss: 0.1799\n",
      "Epoch [510/1000], Loss: 0.1796\n",
      "Epoch [520/1000], Loss: 0.2829\n",
      "Epoch [530/1000], Loss: 0.2289\n",
      "Epoch [540/1000], Loss: 0.3148\n",
      "Epoch [550/1000], Loss: 0.2280\n",
      "Epoch [560/1000], Loss: 0.1470\n",
      "Epoch [570/1000], Loss: 0.2783\n",
      "Epoch [580/1000], Loss: 0.2274\n",
      "Epoch [590/1000], Loss: 0.1862\n",
      "Epoch [600/1000], Loss: 0.2808\n",
      "Epoch [610/1000], Loss: 0.1431\n",
      "Epoch [620/1000], Loss: 0.3090\n",
      "Epoch [630/1000], Loss: 0.2237\n",
      "Epoch [640/1000], Loss: 0.2270\n",
      "Epoch [650/1000], Loss: 0.3610\n",
      "Epoch [660/1000], Loss: 0.2263\n",
      "Epoch [670/1000], Loss: 0.2764\n",
      "Epoch [680/1000], Loss: 0.1817\n",
      "Epoch [690/1000], Loss: 0.3146\n",
      "Epoch [700/1000], Loss: 0.3625\n",
      "Epoch [710/1000], Loss: 0.3591\n",
      "Epoch [720/1000], Loss: 0.1423\n",
      "Epoch [730/1000], Loss: 0.2250\n",
      "Epoch [740/1000], Loss: 0.4386\n",
      "Epoch [750/1000], Loss: 0.1443\n",
      "Epoch [760/1000], Loss: 0.2274\n",
      "Epoch [770/1000], Loss: 0.2272\n",
      "Epoch [780/1000], Loss: 0.0960\n",
      "Epoch [790/1000], Loss: 0.0928\n",
      "Epoch [800/1000], Loss: 0.2270\n",
      "Epoch [810/1000], Loss: 0.2236\n",
      "Epoch [820/1000], Loss: 0.3056\n",
      "Epoch [830/1000], Loss: 0.3531\n",
      "Epoch [840/1000], Loss: 0.3555\n",
      "Epoch [850/1000], Loss: 0.3582\n",
      "Epoch [860/1000], Loss: 0.1425\n",
      "Epoch [870/1000], Loss: 0.1419\n",
      "Epoch [880/1000], Loss: 0.0927\n",
      "Epoch [890/1000], Loss: 0.2249\n",
      "Epoch [900/1000], Loss: 0.3039\n",
      "Epoch [910/1000], Loss: 0.2219\n",
      "Epoch [920/1000], Loss: 0.1758\n",
      "Epoch [930/1000], Loss: 0.2727\n",
      "Epoch [940/1000], Loss: 0.1390\n",
      "Epoch [950/1000], Loss: 0.1757\n",
      "Epoch [960/1000], Loss: 0.2207\n",
      "Epoch [970/1000], Loss: 0.1384\n",
      "Epoch [980/1000], Loss: 0.2713\n",
      "Epoch [990/1000], Loss: 0.1785\n",
      "Epoch [1000/1000], Loss: 0.1380\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 1000 epochs\n",
    "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions: tensor([[0.4796],\n",
      "        [0.6554],\n",
      "        [0.9944],\n",
      "        [0.0032],\n",
      "        [0.9824],\n",
      "        [0.4796],\n",
      "        [0.6554],\n",
      "        [0.9944],\n",
      "        [0.0032],\n",
      "        [0.9824],\n",
      "        [0.4796],\n",
      "        [0.6554],\n",
      "        [0.9944],\n",
      "        [0.0032],\n",
      "        [0.9824]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    final_preds = model(inputs)\n",
    "    print(\"Final Predictions:\", final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут вже ситуація краща, виглядає так що моделька потрохи навчилась предіктити два класи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
